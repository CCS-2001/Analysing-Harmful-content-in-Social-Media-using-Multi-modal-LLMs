{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb8596b-3521-441b-b0ae-639b6e3b1749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "from utils import retrieve_dataset, display_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804006ca-ef6a-4f98-a4f0-8ba98a9b1b5e",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4143e48c-4bef-4f9d-8a50-642476b7b737",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model.cuda()\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "model_name = 'TinyLlama'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8349c71-9396-4851-9207-4f261407d951",
   "metadata": {},
   "source": [
    "# Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a4eb580-6900-4a10-bc8f-876ec79b2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to format the prompt to be given to a LLM\n",
    "\n",
    "Inputs:\n",
    "    sample (dict): a dictionary containing a single sample from a dataset\n",
    "    category (str): the category of harmful content\n",
    "    fs_all (list): list of few shot prompts of the dataste\n",
    "    cot_prompt (str): the chain of thought prompt of the dataset\n",
    "    num_shot (int): number of shots, default is 0\n",
    "    cot (bool): whether it is chain of thought, default is False\n",
    "\n",
    "Returns:\n",
    "    prompt (str): the prompt to be given to a LLM\n",
    "    label (str): the label of the text\n",
    "\n",
    "'''\n",
    "def format_prompt(sample, category, fs_all, cot_prompt, num_shot=0, cot=False):\n",
    "    text = sample['text']\n",
    "    label = sample['label']\n",
    "    label_dict = {0: 'NO',\n",
    "                 1: 'YES'}\n",
    "    \n",
    "\n",
    "    # define the prompt based on the prompting technique\n",
    "    if num_shot:\n",
    "        fs_prompt = 'Below are a set of example questions along with answers.\\n'\n",
    "        fs_prompt += '\\n\\n'.join(fs_all[0:num_shot])\n",
    "        prompt = f'''{fs_prompt} \n",
    "        \n",
    "Now answer the following question.\n",
    "Question: Is the following text <{text}> considered as {category}? \n",
    "Answer:'''\n",
    "\n",
    "    elif cot:\n",
    "        prompt = f'''Below is an instruction with a question, along with the answer to the question and the rationale.\n",
    "{cot_prompt}\n",
    "\n",
    "Now answer the following question.\n",
    "Question: Is the following text <{text}> considered as {category}?  \n",
    "Answer:'''\n",
    "        \n",
    "    else:     \n",
    "        prompt = f'''Question: Is the following text <{text}> considered as {category}?\n",
    "Answer:'''\n",
    "    \n",
    "    return prompt, label_dict[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13f87b23-4cdd-4b2c-8cf5-31ecd03e3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to inference and evaluate a LLM\n",
    "\n",
    "Inputs:\n",
    "    model_name (str): name of the language model\n",
    "    dataset_name (str): name of the dataset\n",
    "    category (str): the category of harmful content\n",
    "    num_shot (int): the number of shots, default is 0\n",
    "    cot (bool): whether it is chain of thought, default is False\n",
    "    num_examples (int): the number of examples to pass to the LLM, default is None\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\n",
    "'''\n",
    "def llm_inf(model_name, dataset_name, category, num_shot=0, cot=False, num_examples=None):\n",
    "    \n",
    "    # retrieve the dataset details\n",
    "    ds, fs_all, cot_prompt = retrieve_dataset(dataset_name, category, instruct=False)\n",
    "    print('Length of', dataset_name, 'test set:', len(ds))\n",
    "\n",
    "    # if no number of examples are provided, use the size of the test set\n",
    "    if num_examples is None:\n",
    "        num_examples = len(ds)\n",
    "        \n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    # go through each example\n",
    "    for i in range(num_examples):\n",
    "        # format the prompt for the LLM\n",
    "        prompt, label = format_prompt(ds[i], category, fs_all, cot_prompt, num_shot, cot)\n",
    "        \n",
    "        # setting up the inputs and inference the model\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a content moderator that detects whether a piece of text is considered {category} (YES) or not (NO)\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    \n",
    "        input_ids = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(model.device)\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=256,\n",
    "            eos_token_id=terminators,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # extract the response of the model\n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        print('Q', i+1)\n",
    "        # print(prompt)\n",
    "        # print(\"model answer: \", response)\n",
    "\n",
    "        llm_answer = 'NO'\n",
    "\n",
    "        # use regular expression to extract exact answer from the model\n",
    "        match1 = re.findall(r'is considered', response)\n",
    "        match2 = re.findall(r'is not considered', response)\n",
    "        match3 = re.findall(r'Answer:\\s*(YES|NO)', response, re.IGNORECASE)\n",
    "        match4 = re.findall(r'^(yes|no)', response, re.IGNORECASE)\n",
    "        match5 = re.findall(r'cannot be classified', response)\n",
    "        \n",
    "        if match1:\n",
    "            llm_answer = 'YES'\n",
    "        elif match2 or match5:\n",
    "            llm_answer = 'NO'\n",
    "        elif match3:\n",
    "            llm_answer = match3[0].upper()\n",
    "        elif match4:\n",
    "            llm_answer = match4[0].upper()\n",
    "\n",
    "        # print('llm answer:', llm_answer)\n",
    "            \n",
    "        # check if answer from model matches the actual answer\n",
    "        if llm_answer.upper().strip() == label:\n",
    "            correct += 1\n",
    "    \n",
    "        true_labels.append(label)\n",
    "        pred_labels.append(llm_answer)\n",
    "\n",
    "    # display the metric scores when all samples have been run\n",
    "    print()\n",
    "    display_results(true_labels, pred_labels, ['YES', 'NO'], model_name, dataset_name, num_shot, cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec66b77-8f42-4b22-8e2f-652ee736cf6c",
   "metadata": {},
   "source": [
    "# HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f491523a-37d4-4f1d-8243-a9d8b052ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'HateXplain'\n",
    "category = 'hate speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d97fb9-347b-40df-85fd-e9562f0b49c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f4ba5a-0b02-4b49-81c3-fe22eb0e4b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77191e45-610a-4a9c-afaa-90624a2a006c",
   "metadata": {},
   "source": [
    "# Toraman hate speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "073643cc-b5b5-4bfa-86a8-d800bd7bb88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Toraman hate speech'\n",
    "category = 'hate speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11197c85-ad31-4542-9dbb-9c3e8a2d8e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e185166c-d7b2-4efd-bdd1-45d8b34a9b6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e12d1e-f5ab-4762-8880-b17a97d15c2e",
   "metadata": {},
   "source": [
    "# OLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5558bd07-1dab-4381-8f1f-a70b0162188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'OLID'\n",
    "category = 'offensive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b85bc-ecbb-4390-b6f1-e5f12580f4b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d6e7d7-b9da-489c-a61e-21631d06af89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4956e3-3573-47fc-a6d8-389818132dab",
   "metadata": {},
   "source": [
    "# OffensEval-TR 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f06ac318-fcb2-4f4e-b888-d5c3ef22846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'OffensEval-TR 2020'\n",
    "category = 'offensive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0345d0f-1ad5-4eae-ab19-7a1107ece82d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d6904-7fce-4e96-b547-e9fab399a4d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71455f8-23b2-443b-af82-d7f4fcc352e5",
   "metadata": {},
   "source": [
    "# Toxigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb3dced-bc3d-4fa1-97d4-d7e894c29322",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Toxigen'\n",
    "category = 'toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acd1e7-78a9-41a4-9c03-70702873032e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a5f7f-7a78-4975-9ba2-14d193c1519e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a12a3f-6444-4771-90c1-35f8ee40e044",
   "metadata": {},
   "source": [
    "# LLM-JP Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb963b22-3577-4d49-9c22-b3fcc2435ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'LLM-JP Toxicity'\n",
    "category = 'toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfa366d-c9be-4dd9-8a01-103e0e5dbef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca7a44-a55c-45e6-beac-4891c21f5abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a84b618-ca9d-4a88-a7e4-14929da6e9a1",
   "metadata": {},
   "source": [
    "# Ejaz cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e436fc2-3048-43fe-b840-e2472e6d6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Ejaz cyberbullying'\n",
    "category = 'cyberbullying'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8713126-02db-4279-840d-d94916c41389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6fe926-07dc-4180-9e73-4f6d6acfb826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b771287-ef69-43b1-9964-c5d8604e57ae",
   "metadata": {},
   "source": [
    "# SOSNet cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f44dd8d-6316-46b6-bf4b-f22134fc26e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'SOSNet cyberbullying'\n",
    "category = 'cyberbullying'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ca61c-b277-4828-828a-7857b6d63be4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5eb05-58ac-4a88-8947-b7c3d367f652",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
