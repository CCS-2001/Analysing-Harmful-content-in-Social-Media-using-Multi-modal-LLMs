{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7601967-ba4e-4e97-8615-4dc8f492917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import re\n",
    "from utils import retrieve_dataset, display_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a379bf4-9db3-4b71-aee7-b32c30c32c94",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1b537eb-6b17-4f12-9f5d-4a06d6cb2ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2-1.5B-Instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-1.5B-Instruct\")\n",
    "model.cuda()\n",
    "model_name = 'Qwen2-1.5B'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3748d-9e97-4f30-bb95-a8e4113596b6",
   "metadata": {},
   "source": [
    "# Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89dca884-b79c-4022-a654-e0664bd0bc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to format the prompt to be given to a LLM\n",
    "\n",
    "Inputs:\n",
    "    sample (dict): a dictionary containing a single sample from a dataset\n",
    "    category (str): the category of harmful content\n",
    "    fs_all (list): list of few shot prompts of the dataste\n",
    "    cot_prompt (str): the chain of thought prompt of the dataset\n",
    "    num_shot (int): number of shots, default is 0\n",
    "    cot (bool): whether it is chain of thought, default is False\n",
    "\n",
    "Returns:\n",
    "    prompt (str): the prompt to be given to a LLM\n",
    "    label (str): the label of the text\n",
    "\n",
    "'''\n",
    "def format_prompt(sample, category, fs_all, cot_prompt, num_shot=0, cot=False):\n",
    "    text = '###TEXT: ' + sample['text']\n",
    "    label = sample['label']\n",
    "    label_dict = {0: 'NO',\n",
    "                 1: 'YES'}\n",
    "\n",
    "    # define the instruction\n",
    "    if cot:\n",
    "        instruction = f'###INSTRUCTION: Given the following text, classify it as either {category} (YES) or not (NO). \\\n",
    "Provide a rationale to your answer then give the answer as YES or NO. You must give a YES or NO answer.'\n",
    "    else:\n",
    "        instruction = f'###INSTRUCTION: Given the following text, classify it as either {category} (YES) or not (NO). \\\n",
    "If it is {category}, answer YES. If it is not {category}, answer NO. You must answer with only YES or NO.'    \n",
    "\n",
    "    # define the prompt based on the prompting technique\n",
    "    if num_shot:\n",
    "        fs_prompt = 'Below are a set of instructions with a question, along with answers.\\n'\n",
    "        fs_prompt += '\\n\\n'.join(fs_all[0:num_shot])\n",
    "        prompt = f'''{fs_prompt} \n",
    "        \n",
    "Now answer the following question.\n",
    "{instruction}\n",
    "{text}\n",
    "###ANSWER:'''\n",
    "  \n",
    "    elif cot:\n",
    "        prompt = f'''Below is an instruction with a question, along with the answer to the question and the rationale.\n",
    "{instruction}\n",
    "{cot_prompt}\n",
    "\n",
    "Now answer the following question by giving a rationale and answer.\n",
    "{instruction}\n",
    "{text}'''\n",
    "        \n",
    "    else:     \n",
    "        prompt = f'''{instruction}\n",
    "{text}\n",
    "###ANSWER:'''\n",
    "    \n",
    "    return prompt, label_dict[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f5f0c7-14ce-4bbc-8106-7520bd56f12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to inference and evaluate a LLM\n",
    "\n",
    "Inputs:\n",
    "    model_name (str): name of the language model\n",
    "    dataset_name (str): name of the dataset\n",
    "    category (str): the category of harmful content\n",
    "    num_shot (int): the number of shots, default is 0\n",
    "    cot (bool): whether it is chain of thought, default is False\n",
    "    num_examples (int): the number of examples to pass to the LLM, default is None\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\n",
    "'''\n",
    "def llm_inf(model_name, dataset_name, category, num_shot=0, cot=False, num_examples=None):\n",
    "    \n",
    "    # retrieve the dataset details\n",
    "    ds, fs_all, cot_prompt = retrieve_dataset(dataset_name, category)\n",
    "    print('Length of', dataset_name, 'test set:', len(ds))\n",
    "\n",
    "    # if no number of examples are provided, use the size of the test set\n",
    "    if num_examples is None:\n",
    "        num_examples = len(ds)\n",
    "        \n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    # go through each example\n",
    "    for i in range(num_examples):\n",
    "        # format the prompt for the LLM\n",
    "        prompt, label = format_prompt(ds[i], category, fs_all, cot_prompt, num_shot, cot)\n",
    "        \n",
    "        # setting up the inputs and inference the model\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a content moderator that detects whether a piece of text is considered {category} (YES) or not (NO)\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    \n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_k=None,\n",
    "            top_p=None\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "    \n",
    "        # extract the response of the model\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        print('Q', i+1)\n",
    "        # print(prompt)\n",
    "        # print(\"model answer: \", response)\n",
    "\n",
    "        llm_answer = 'NO'\n",
    "\n",
    "        # use regular expression to extract exact answer from the model\n",
    "        match1 = re.findall(r'\\b(yes|no)\\b', response, re.IGNORECASE)\n",
    "        match2 = re.findall(r'cannot (moderate|classify)', response)\n",
    "\n",
    "        if match1:\n",
    "            llm_answer = match1[0].upper()\n",
    "        elif match2:\n",
    "            llm_answer = 'YES'\n",
    "\n",
    "        # print('llm answer:', llm_answer)\n",
    "            \n",
    "        # check if answer from model matches the actual answer\n",
    "        if llm_answer.upper().strip() == label:\n",
    "            correct += 1\n",
    "    \n",
    "        true_labels.append(label)\n",
    "        pred_labels.append(llm_answer)\n",
    "\n",
    "    # display the metric scores when all samples have been run\n",
    "    print()\n",
    "    display_results(true_labels, pred_labels, ['YES', 'NO'], model_name, dataset_name, num_shot, cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23b858-33ba-49b9-a6d7-a580d8ebaafa",
   "metadata": {},
   "source": [
    "# HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19beb515-2820-4646-8fd8-a42044072540",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'HateXplain'\n",
    "category = 'hate speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6b2c0-4919-44a2-b3fb-3f59f47ee55f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3446dcf-ed8d-4e26-beac-2bf676c75999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe60c3-a4eb-4066-8e85-3e00885755be",
   "metadata": {},
   "source": [
    "# Toraman hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c7b06dc-c02f-404d-87e0-d4bbdc70759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Toraman hate speech'\n",
    "category = 'hate speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d07eb4-86a7-4855-93d5-d2d671673439",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aafbbbb-ad5e-4b98-a0b2-54fd54169980",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc601cf1-d7d5-4d66-9672-6f6e4d2c88bf",
   "metadata": {},
   "source": [
    "# OLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0eeadbc7-eb86-4676-be44-3ac01b7fc719",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'OLID'\n",
    "category = 'offensive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06527c32-c2c1-47df-8f64-970b1de1163a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0784915-e521-4377-a37c-0084f7eab003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa74678-0634-49e1-8161-0cab22ea2e32",
   "metadata": {},
   "source": [
    "# Offenseval2020_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87f88c4d-4dd5-4ba3-951b-dfee22b136f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'OffensEval-TR 2020'\n",
    "category = 'offensive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032ae792-3172-4621-9d24-f600efd2a3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f71f5d-0722-4302-9076-3d258c2a8fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c7ce5-6be9-4829-b85e-2e4e32e58e04",
   "metadata": {},
   "source": [
    "# Toxigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aa8a111-c45b-4ed1-9018-311b795c754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Toxigen'\n",
    "category = 'toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84e7f8-922a-450d-9b21-8baa08562a26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d940e4e-5407-4ad0-a9d6-5eacb04c7239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaf24b8-de7a-42d5-b713-5045520bddf6",
   "metadata": {},
   "source": [
    "# LLM-JP Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207bb1d4-199c-4251-81d1-ec2f35766e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'LLM-JP Toxicity'\n",
    "category = 'toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbba778-c9ee-46c0-82a5-06e2951c8f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee0640-9516-4dad-8b6d-2697804e35f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb414a5-7fb4-4e0e-9270-a8dcfec6f653",
   "metadata": {},
   "source": [
    "# Ejaz cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5b3e75f-1181-44ab-8797-412d4c841783",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Ejaz cyberbullying'\n",
    "category = 'cyberbullying'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eecf2e4-5b2d-4aa8-8971-8009d7dee124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5c953-bee9-4c1e-8aa9-d41430ec2785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03059ca8-c70a-41b2-a3b0-6d144b7a780e",
   "metadata": {},
   "source": [
    "# SOSNet cyberbullying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161d3d73-e2cb-44e5-9cfa-e47692e52c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'SOSNet cyberbullying'\n",
    "category = 'cyberbullying'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513597a-977c-4877-9232-7a1e917994b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e87119-b7e1-4db8-8566-1b6fbc2a62e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
