{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0062a3a2-0d60-4a14-ba97-550a99e2c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "from utils import retrieve_dataset, display_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c445c1-c4d4-437f-a300-e056f58d55c0",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d4c146-26a1-476e-8d78-701f44127a8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MBZUAI/MobiLlama-05B-Chat\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"MBZUAI/MobiLlama-05B-Chat\", trust_remote_code=True).cuda()\n",
    "model_name = 'MobiLlama'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc01b01-0b4f-4b32-ad5c-adee4dc832f0",
   "metadata": {},
   "source": [
    "# Inference function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7903a173-8287-4660-a39f-f6563a67505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to format the prompt to be given to a LLM\n",
    "\n",
    "Inputs:\n",
    "    sample (dict): a dictionary containing a single sample from a dataset\n",
    "    category (str): the category of harmful content\n",
    "    fs_all (list): list of few shot prompts of the dataste\n",
    "    cot_prompt (str): the chain of thought prompt of the dataset\n",
    "    num_shot (int): number of shots, default is 0\n",
    "    cot (bool): whether it is chain of thought, default is False\n",
    "\n",
    "Returns:\n",
    "    prompt (str): the prompt to be given to a LLM\n",
    "    label (str): the label of the text\n",
    "\n",
    "'''\n",
    "def format_prompt(sample, category, fs_all, cot_prompt, num_shot=0, cot=False):\n",
    "    text = sample['text']\n",
    "    label = sample['label']\n",
    "    label_dict = {0: 'NO',\n",
    "                 1: 'YES'}\n",
    "\n",
    "    # define the prompt based on the prompting technique\n",
    "    if num_shot:\n",
    "        fs_prompt = 'Below are a set of example questions along with answers.\\n'\n",
    "        fs_prompt += '\\n\\n'.join(fs_all[0:num_shot])\n",
    "        prompt = f'''{fs_prompt} \n",
    "        \n",
    "Now answer the following question.\n",
    "Question: Is the following text <{text}> considered as {category}?  \n",
    "Answer:'''\n",
    "\n",
    "    elif cot:\n",
    "        prompt = f'''Below is with a question, along with the answer to the question and the rationale.\n",
    "{cot_prompt}\n",
    "\n",
    "Now answer the following question.\n",
    "Question: Is the following text <{text}> considered as {category}?  \n",
    "Answer:'''\n",
    "        \n",
    "    else:     \n",
    "        prompt = f'''Question: Is the following text <{text}> considered as {category}? \n",
    "Answer:'''\n",
    "    \n",
    "    return prompt, label_dict[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9d1b45-1d3a-4f8e-81da-278054c012c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to inference and evaluate a LLM\n",
    "\n",
    "Inputs:\n",
    "    model_name (str): name of the language model\n",
    "    dataset_name (str): name of the dataset\n",
    "    category (str): the category of harmful content\n",
    "    num_shot (int): the number of shots, default is 0\n",
    "    cot (bool): whether it is chain of thought, default is False\n",
    "    num_examples (int): the number of examples to pass to the LLM, default is None\n",
    "\n",
    "Returns:\n",
    "    None\n",
    "\n",
    "'''\n",
    "def llm_inf(model_name, dataset_name, category, num_shot=0, cot=False, num_examples=None):\n",
    "    \n",
    "    # retrieve the dataset details\n",
    "    ds, fs_all, cot_prompt = retrieve_dataset(dataset_name, category, instruct=False)\n",
    "    print('Length of', dataset_name, 'test set:', len(ds))\n",
    "\n",
    "    # if no number of examples are provided, use the size of the test set\n",
    "    if num_examples is None:\n",
    "        num_examples = len(ds)\n",
    "        \n",
    "    correct = 0\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    # go through each example\n",
    "    for i in range(num_examples):\n",
    "        # format the prompt for the LLM\n",
    "        prompt, label = format_prompt(ds[i], category, fs_all, cot_prompt, num_shot, cot)\n",
    "    \n",
    "        # setting up the inputs and inference the model\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda').input_ids\n",
    "        outputs = model.generate(input_ids, \n",
    "                                max_new_tokens=150, \n",
    "                                pad_token_id=tokenizer.eos_token_id,\n",
    "                                do_sample=False,\n",
    "                                temperature=None,\n",
    "                                top_p=None,\n",
    "                                top_k=None)\n",
    "        \n",
    "        # extract the response of the model\n",
    "        response = tokenizer.batch_decode(outputs[:, input_ids.shape[1]:-1])[0].strip()\n",
    "        print('Q', i+1)\n",
    "        # print(prompt)\n",
    "        # print(\"model answer: \", response)\n",
    "\n",
    "        llm_answer = 'NO'\n",
    "\n",
    "        # use regular expression to extract exact answer from the model\n",
    "        match1 = re.findall(r'^(yes|no)', response, re.IGNORECASE)\n",
    "        match2 = re.findall(r'is considered', response)\n",
    "        match3 = re.findall(r'is not considered', response)\n",
    "\n",
    "        if match1:\n",
    "            llm_answer = match1[0].upper()\n",
    "        elif match2:\n",
    "            llm_answer = 'YES'\n",
    "        elif match3:\n",
    "            llm_answer = 'NO'\n",
    "            \n",
    "        # print('llm answer:', llm_answer)\n",
    "            \n",
    "        # check if answer from model matches the actual answer\n",
    "        if llm_answer.upper().strip() == label:\n",
    "            correct += 1\n",
    "    \n",
    "        true_labels.append(label)\n",
    "        pred_labels.append(llm_answer)\n",
    "\n",
    "    # display the metric scores when all samples have been run\n",
    "    print()\n",
    "    display_results(true_labels, pred_labels, ['YES', 'NO'], model_name, dataset_name, num_shot, cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbd00e-106d-489b-baf6-a43601ac1628",
   "metadata": {},
   "source": [
    "# HateXplain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61606ef-7ef8-4d62-9756-863efed12eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'HateXplain'\n",
    "category = 'hate speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9835949-e161-4aa5-b473-1d68ea2a18c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b2ae4-f117-44d6-85d9-d0f88170d21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18670c-78eb-4b6e-bce1-3b10650b1662",
   "metadata": {},
   "source": [
    "# Toraman hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cf77044-eead-4de8-95db-1c99a07f162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Toraman hate speech'\n",
    "category = 'hate speech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b3cca-1cad-497c-85c8-9dcf9771869c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907aa379-eba0-490e-ba60-8c362631f4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d3e58-c6f2-445d-a3db-b015f566cbac",
   "metadata": {},
   "source": [
    "# OLID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d470cde-1009-4145-898d-47ed2ce28d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'OLID'\n",
    "category = 'offensive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47c7a8-c50f-4395-9c29-22bfd9a16375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765aad9-b1e4-4bcb-999c-738e112011f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0837a43d-b14b-4116-9f36-fe6ca18e430f",
   "metadata": {},
   "source": [
    "# OffensEval-TR 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7fd9343-78bf-49c9-a373-053e1be287f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'OffensEval-TR 2020'\n",
    "category = 'offensive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cce6f-6f17-4022-aa89-f0ed57d456db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c37d0e-4f3e-4785-8a62-666fe580ec0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134463ca-3e65-49e1-a60e-ad1321f24fe5",
   "metadata": {},
   "source": [
    "# Toxigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ecf15e-a2db-4bdc-9c44-96ef3fd1dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Toxigen'\n",
    "category = 'toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc76694-f619-4980-aaee-69b96e6aea6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b96c24-c4ef-4ca1-a39d-2133e3877c13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d55e7-7340-4168-846a-d64e4d0be201",
   "metadata": {},
   "source": [
    "# LLM-JP Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45668d6f-020e-4aae-b482-71f7c4c8da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'LLM-JP Toxicity'\n",
    "category = 'toxic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec199ce-c1cd-4d3d-891c-22ec9c0317d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc002e-fafb-4150-9a92-db4d19d12e30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88face88-6fad-4a8c-b88f-ddd68882f65c",
   "metadata": {},
   "source": [
    "# Ejaz cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3bc5d2c-795d-4592-9d65-4579ba9ae210",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Ejaz cyberbullying'\n",
    "category = 'cyberbullying'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe0943-0279-4db8-8863-3ff016aa870f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c80f1-b12d-468b-ac5e-a3186874b1a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d91b1c-7a28-43db-8661-fd861f8f0411",
   "metadata": {},
   "source": [
    "# SOSNet Cyberbullying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21971e96-7f4a-4ac6-a150-fd1dee456b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'SOSNet cyberbullying'\n",
    "category = 'cyberbullying'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6929dc19-4fba-4150-ab04-dd32f13d539b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Zero-shot, one-shot and two-shot\n",
    "for i in range(3):\n",
    "    num_shot = i\n",
    "    llm_inf(model_name, dataset_name, category, num_shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ad90d-8c14-4f8d-9761-1d4d000bbb75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Chain of thought\n",
    "llm_inf(model_name, dataset_name, category, cot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
